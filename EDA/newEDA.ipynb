{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Adjust this path if your file is in a different location\n",
    "DATA_PATH = '../data/train.csv'\n",
    "\n",
    "try:\n",
    "    print(\"Loading Dataset...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Data Loaded Successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {DATA_PATH}. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: NUMERICAL DISTRIBUTION & SKEW (The \"MRI\")\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"      PART 1: NUMERICAL DISTRIBUTION & SKEW ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter numerical columns (excluding ID)\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "num_cols = [c for c in num_cols if c != 'founder_id']\n",
    "\n",
    "# Create summary table\n",
    "dist_summary = pd.DataFrame(index=num_cols)\n",
    "dist_summary['Skewness'] = df[num_cols].skew()\n",
    "dist_summary['Kurtosis'] = df[num_cols].kurt()\n",
    "dist_summary['Null_Count'] = df[num_cols].isnull().sum()\n",
    "\n",
    "# Recommendation logic\n",
    "def recommend_transform(row):\n",
    "    if abs(row['Skewness']) > 1:\n",
    "        return \"Log/Power Transform Needed (High Skew)\"\n",
    "    elif abs(row['Skewness']) > 0.5:\n",
    "        return \"Moderate Skew (Consider Scaling)\"\n",
    "    else:\n",
    "        return \"Normal-ish (StandardScaler ok)\"\n",
    "\n",
    "dist_summary['Recommendation'] = dist_summary.apply(recommend_transform, axis=1)\n",
    "\n",
    "print(\"\\nDISTRIBUTION HEALTH CHECK:\")\n",
    "print(dist_summary.sort_values(by='Skewness', key=abs, ascending=False))\n",
    "\n",
    "# --- VISUALIZATION: Histograms + Q-Q Plots ---\n",
    "# Identify skewed columns for plotting\n",
    "skewed_cols = dist_summary[dist_summary['Recommendation'].str.contains(\"Transform\")].index.tolist()\n",
    "\n",
    "if len(skewed_cols) > 0:\n",
    "    print(f\"\\n[Visualizing Top Skewed Features]: {skewed_cols}\")\n",
    "    # Create a dynamic subplot layout\n",
    "    rows = len(skewed_cols)\n",
    "    plt.figure(figsize=(14, 5 * rows))\n",
    "    \n",
    "    for i, col in enumerate(skewed_cols):\n",
    "        # 1. Histogram\n",
    "        ax1 = plt.subplot(rows, 2, i*2 + 1)\n",
    "        sns.histplot(df[col].dropna(), kde=True, color='purple', ax=ax1)\n",
    "        ax1.set_title(f'{col} Distribution (Skew: {df[col].skew():.2f})')\n",
    "        \n",
    "        # 2. Q-Q Plot\n",
    "        ax2 = plt.subplot(rows, 2, i*2 + 2)\n",
    "        stats.probplot(df[col].dropna(), dist=\"norm\", plot=ax2)\n",
    "        ax2.set_title(f'{col} Q-Q Plot (Normality Check)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo heavily skewed columns found (Skewness < 1.0).\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: CATEGORICAL PREDICTIVE POWER (Cramer's V)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"      PART 2: CATEGORICAL FEATURE STRENGTH (CRAMER'S V)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"Calculates Cramer's V statistic for categorical-categorical association.\"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    \n",
    "    # Handle division by zero edge cases\n",
    "    if min((kcorr-1), (rcorr-1)) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "# Get categorical columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Calculate Cramer's V for each column against 'retention_status'\n",
    "correlations = {}\n",
    "print(\"Calculating correlations...\")\n",
    "for col in cat_cols:\n",
    "    if col != 'retention_status':\n",
    "        # Drop NaNs purely for this calculation to avoid errors\n",
    "        clean_data = df[[col, 'retention_status']].dropna()\n",
    "        # Only calculate if we have data\n",
    "        if not clean_data.empty:\n",
    "            score = cramers_v(clean_data[col], clean_data['retention_status'])\n",
    "            correlations[col] = score\n",
    "\n",
    "# Create DataFrame & Sort\n",
    "cat_corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Cramers_V'])\n",
    "cat_corr_df = cat_corr_df.sort_values(by='Cramers_V', ascending=False)\n",
    "\n",
    "print(\"\\nSTRENGTH OF ASSOCIATION WITH TARGET (0.0 to 1.0):\")\n",
    "print(cat_corr_df)\n",
    "\n",
    "# --- VISUALIZATION: Feature Importance Bar Plot ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "# FIX: Added hue and legend=False to fix the FutureWarning\n",
    "sns.barplot(\n",
    "    x=cat_corr_df.Cramers_V, \n",
    "    y=cat_corr_df.index, \n",
    "    hue=cat_corr_df.index, \n",
    "    legend=False, \n",
    "    palette='magma'\n",
    ")\n",
    "plt.title(\"Which Categorical Features Matter Most? (Cramer's V)\")\n",
    "plt.xlabel(\"Cramer's V Score (0=Noise, 1=Perfect Predictor)\")\n",
    "plt.axvline(x=0.05, color='red', linestyle='--', label='Noise Threshold (0.05)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: MULTIVARIATE INTERACTIONS (3D Analysis)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"      PART 3: MULTIVARIATE INTERACTIONS & PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Scatter: Age vs Revenue vs Retention\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df, \n",
    "    x='founder_age', \n",
    "    y='monthly_revenue_generated', \n",
    "    hue='retention_status', \n",
    "    alpha=0.6,\n",
    "    palette='coolwarm'\n",
    ")\n",
    "plt.title('Interaction: Does High Revenue keep Young Founders?')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Violin Plot: Satisfaction vs Revenue\n",
    "# Check if \"Money\" (Revenue) compensates for \"Happiness\" (Satisfaction)\n",
    "if 'venture_satisfaction' in df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(\n",
    "        data=df, \n",
    "        x='venture_satisfaction', \n",
    "        y='monthly_revenue_generated', \n",
    "        hue='retention_status',\n",
    "        split=True,\n",
    "        order=['Low', 'Medium', 'High', 'Very High'], # Adjust if your categories differ\n",
    "        palette='muted'\n",
    "    )\n",
    "    plt.title('Distribution of Revenue by Satisfaction & Retention Status')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Pairplot for Key Numerical Factors\n",
    "# A quick glance at the relationships between the most important numeric vars\n",
    "key_numeric = ['founder_age', 'years_with_startup', 'monthly_revenue_generated', 'retention_status']\n",
    "# Ensure columns exist before plotting\n",
    "plot_cols = [c for c in key_numeric if c in df.columns]\n",
    "\n",
    "if len(plot_cols) > 1:\n",
    "    print(\"Generating Pairplot... (This might take a moment)\")\n",
    "    sns.pairplot(df[plot_cols], hue='retention_status', palette='husl', corner=True)\n",
    "    plt.suptitle(\"Pairwise Relationships of Key Numerical Features\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"      EXTENSIVE EDA COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d460126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.max_open_warning'] = 50 # Allow many plots\n",
    "\n",
    "# Load Data\n",
    "possible_paths = ['../data/train.csv', 'train.csv']\n",
    "DATA_PATH = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        DATA_PATH = path\n",
    "        break\n",
    "\n",
    "if DATA_PATH is None:\n",
    "    print(\"❌ Error: 'train.csv' not found.\")\n",
    "else:\n",
    "    print(f\"✅ Data Loaded from {DATA_PATH}\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 1. THE MASTER CORRELATION HEATMAP (Numerical Only)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      1. MASTER CORRELATION HEATMAP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Select only numbers\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "    # Drop ID if present\n",
    "    if 'founder_id' in numeric_df.columns:\n",
    "        numeric_df = numeric_df.drop(columns=['founder_id'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # mask=np.triu(...) hides the upper triangle (since it's a mirror image)\n",
    "    mask = np.triu(np.ones_like(numeric_df.corr(), dtype=bool))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        numeric_df.corr(), \n",
    "        mask=mask,\n",
    "        annot=True, \n",
    "        fmt=\".2f\", \n",
    "        cmap='coolwarm', \n",
    "        linewidths=0.5,\n",
    "        vmin=-1, vmax=1\n",
    "    )\n",
    "    plt.title('Correlation Matrix of All Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 2. NUMERICAL FEATURES vs TARGET (Boxplots)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      2. COMPARING NUMERICAL INPUTS WITH OUTPUT\")\n",
    "    print(\"      (Look for shift in the box/median)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Get list of numerical cols again\n",
    "    num_cols = numeric_df.columns.tolist()\n",
    "\n",
    "    # Create a grid of plots\n",
    "    # We will do 3 plots per row\n",
    "    n_cols = 3\n",
    "    n_rows = (len(num_cols) - 1) // n_cols + 1\n",
    "    \n",
    "    plt.figure(figsize=(15, 4 * n_rows))\n",
    "    \n",
    "    for i, col in enumerate(num_cols):\n",
    "        ax = plt.subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        # Boxplot: Great for seeing if the \"Average\" or \"Spread\" is different\n",
    "        sns.boxplot(\n",
    "            data=df, \n",
    "            x='retention_status', \n",
    "            y=col, \n",
    "            hue='retention_status', \n",
    "            palette='Set2', \n",
    "            legend=False,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(f'{col} vs Retention')\n",
    "        ax.set_xlabel('')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 3. CATEGORICAL FEATURES vs TARGET (Stacked Bar Charts)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      3. COMPARING CATEGORICAL INPUTS WITH OUTPUT\")\n",
    "    print(\"      (Look for bars with different Red/Blue proportions)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    # Remove target from inputs\n",
    "    cat_cols = [c for c in cat_cols if c != 'retention_status']\n",
    "\n",
    "    # Filter out columns with too many categories (like names or IDs)\n",
    "    cat_cols = [c for c in cat_cols if df[c].nunique() < 20]\n",
    "\n",
    "    for col in cat_cols:\n",
    "        # Create a Crosstab (Contingency Table)\n",
    "        cross_tab = pd.crosstab(df[col], df['retention_status'])\n",
    "        \n",
    "        # Calculate Proportions (Rows sum to 100%)\n",
    "        # This is better than counts because it handles unequal group sizes\n",
    "        cross_tab_prop = cross_tab.div(cross_tab.sum(1), axis=0)\n",
    "        \n",
    "        # Plot\n",
    "        ax = cross_tab_prop.plot(\n",
    "            kind='bar', \n",
    "            stacked=True, \n",
    "            figsize=(10, 4), \n",
    "            colormap='viridis',\n",
    "            edgecolor='black'\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Retention Rates by {col} (Normalized)', fontsize=14)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.legend(title='Retention Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Add text labels for percentages\n",
    "        for p in ax.patches:\n",
    "            width, height = p.get_width(), p.get_height()\n",
    "            x, y = p.get_xy() \n",
    "            if height > 0.05: # Only show text if segment is big enough\n",
    "                ax.text(x+width/2, \n",
    "                        y+height/2, \n",
    "                        '{:.0f}%'.format(height*100), \n",
    "                        horizontalalignment='center', \n",
    "                        verticalalignment='center',\n",
    "                        color='white',\n",
    "                        weight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3015c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Applying Forensic Cleaning Pipeline...\n",
      "Scaling Data...\n",
      "\n",
      "==================================================\n",
      "   STARTING MULTI-MODEL GENERATION\n",
      "==================================================\n",
      "\n",
      "Processing Model: GradientBoosting...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "  -> Best Params: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "  -> Best CV Score: 75.3334%\n",
      "  -> Saved to '../output/submission_GradientBoosting.csv' (Time: 37.0s)\n",
      "\n",
      "Processing Model: RandomForest...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "  -> Best Params: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 10}\n",
      "  -> Best CV Score: 74.7094%\n",
      "  -> Saved to '../output/submission_RandomForest.csv' (Time: 17.4s)\n",
      "\n",
      "Processing Model: SVM...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Best Params: {'kernel': 'rbf', 'gamma': 'scale', 'C': 1}\n",
      "  -> Best CV Score: 74.2346%\n",
      "  -> Saved to '../output/submission_SVM.csv' (Time: 236.3s)\n",
      "\n",
      "Processing Model: NeuralNetwork...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MlLab/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Best Params: {'learning_rate_init': 0.001, 'hidden_layer_sizes': (50,), 'alpha': 0.001, 'activation': 'relu'}\n",
      "  -> Best CV Score: 74.0769%\n",
      "  -> Saved to '../output/submission_NeuralNetwork.csv' (Time: 62.2s)\n",
      "\n",
      "==================================================\n",
      "   ALL FILES GENERATED IN ../output/\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_DIR = '../output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) # Create folder if it doesn't exist\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "print(\"Loading Data...\")\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# Combine for consistent processing\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test['retention_status'] = 'Unknown'\n",
    "combined = pd.concat([train, test], axis=0)\n",
    "\n",
    "# --- 2. FORENSIC PRE-PROCESSING ---\n",
    "print(\"Applying Forensic Cleaning Pipeline...\")\n",
    "\n",
    "# A. DROP THE NOISE (Your 'Kill List')\n",
    "cols_to_drop = ['founder_id', 'founder_role', 'leadership_scope', \n",
    "                'founder_visibility', 'innovation_support', 'team_size_category']\n",
    "combined = combined.drop(columns=cols_to_drop)\n",
    "\n",
    "# B. FIX SKEW\n",
    "combined['monthly_revenue_generated'] = np.log1p(combined['monthly_revenue_generated'].fillna(combined['monthly_revenue_generated'].median()))\n",
    "\n",
    "# C. IMPUTE MISSING VALUES\n",
    "combined['years_since_founding'] = combined['years_since_founding'].fillna(combined['years_since_founding'].median())\n",
    "combined['num_dependents'] = combined['num_dependents'].fillna(combined['num_dependents'].mode()[0])\n",
    "combined['work_life_balance_rating'] = combined['work_life_balance_rating'].fillna('Unknown')\n",
    "combined['venture_satisfaction'] = combined['venture_satisfaction'].fillna('Unknown')\n",
    "\n",
    "# D. FEATURE ENGINEERING\n",
    "combined['founder_age'] = combined['founder_age'].clip(lower=18)\n",
    "combined['start_age'] = combined['founder_age'] - combined['years_with_startup']\n",
    "\n",
    "# E. ORDINAL ENCODING\n",
    "rating_map = {'Unknown': 0, 'Low': 1, 'Poor': 1, 'Below Average': 2, \n",
    "              'Fair': 3, 'Medium': 3, 'Average': 3, \n",
    "              'Good': 4, 'High': 4, 'Very High': 5, 'Excellent': 5}\n",
    "for col in ['work_life_balance_rating', 'venture_satisfaction', 'startup_performance_rating', 'startup_reputation']:\n",
    "    combined[col] = combined[col].map(rating_map).fillna(0)\n",
    "\n",
    "stage_map = {'Entry': 1, 'Mid': 2, 'Senior': 3, 'Growth': 3, 'Established': 4}\n",
    "combined['startup_stage'] = combined['startup_stage'].map(stage_map).fillna(1)\n",
    "\n",
    "binary_map = {'No': 0, 'Yes': 1}\n",
    "for col in ['working_overtime', 'remote_operations']:\n",
    "    combined[col] = combined[col].map(binary_map)\n",
    "\n",
    "# F. ONE-HOT ENCODING\n",
    "combined = pd.get_dummies(combined, columns=['founder_gender', 'education_background', 'personal_status'], drop_first=True)\n",
    "\n",
    "# --- 3. PREPARE MATRICES ---\n",
    "print(\"Scaling Data...\")\n",
    "train_final = combined[combined['is_train'] == 1].drop(columns=['is_train'])\n",
    "test_final = combined[combined['is_train'] == 0].drop(columns=['is_train', 'retention_status'])\n",
    "\n",
    "# Target Mapping\n",
    "target_map = {'Stayed': 0, 'Left': 1}\n",
    "y = train_final['retention_status'].map(target_map)\n",
    "X = train_final.drop(columns=['retention_status'])\n",
    "X_submit = test_final[X.columns]\n",
    "\n",
    "# Scale (Critical for SVM/NN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_submit_scaled = scaler.transform(X_submit)\n",
    "\n",
    "inverse_map = {0: 'Stayed', 1: 'Left'}\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. DEFINE MODELS & PARAMETER GRIDS\n",
    "# ==============================================================================\n",
    "models_config = {\n",
    "    \"GradientBoosting\": {\n",
    "        \"estimator\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"estimator\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"estimator\": SVC(random_state=42),\n",
    "        \"params\": {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf'], \n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    \"NeuralNetwork\": {\n",
    "        \"estimator\": MLPClassifier(max_iter=500, random_state=42),\n",
    "        \"params\": {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001],\n",
    "            'learning_rate_init': [0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. EXECUTION LOOP\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"   STARTING MULTI-MODEL GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, config in models_config.items():\n",
    "    print(f\"\\nProcessing Model: {name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # 1. Hyperparameter Tuning (Random Search)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=config['estimator'],\n",
    "        param_distributions=config['params'],\n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_scaled, y)\n",
    "    best_model = search.best_estimator_\n",
    "    \n",
    "    print(f\"  -> Best Params: {search.best_params_}\")\n",
    "    print(f\"  -> Best CV Score: {search.best_score_:.4%}\")\n",
    "    \n",
    "    # 2. Predict on Test Set using the BEST model\n",
    "    preds = best_model.predict(X_submit_scaled)\n",
    "    \n",
    "    # 3. Save CSV to OUTPUT_DIR\n",
    "    filename = f\"{OUTPUT_DIR}/submission_{name}.csv\"\n",
    "    submission = pd.DataFrame({\n",
    "        'founder_id': test['founder_id'],\n",
    "        'retention_status': [inverse_map[p] for p in preds]\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  -> Saved to '{filename}' (Time: {elapsed:.1f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"   ALL FILES GENERATED IN {OUTPUT_DIR}/\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316cd510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions...\n",
      "Loaded: submission_GradientBoosting.csv\n",
      "Loaded: submission_RandomForest.csv\n",
      "Loaded: submission_SVM.csv\n",
      "Loaded: submission_NeuralNetwork.csv\n",
      "\n",
      "✅ Created Ensemble Submission: ../output/submission_Ensemble_Voting.csv\n",
      "This file combines the 'wisdom' of all your models.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import mode\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_DIR = '../output'\n",
    "files = [\n",
    "    'submission_GradientBoosting.csv',\n",
    "    'submission_RandomForest.csv',\n",
    "    'submission_SVM.csv',\n",
    "    'submission_NeuralNetwork.csv'\n",
    "]\n",
    "\n",
    "print(\"Loading predictions...\")\n",
    "dfs = []\n",
    "for f in files:\n",
    "    path = os.path.join(OUTPUT_DIR, f)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        # Convert to numbers for math (Stayed=0, Left=1)\n",
    "        df['retention_numeric'] = df['retention_status'].map({'Stayed': 0, 'Left': 1})\n",
    "        dfs.append(df['retention_numeric'].values)\n",
    "        print(f\"Loaded: {f}\")\n",
    "    else:\n",
    "        print(f\"Warning: {f} not found. Skipping.\")\n",
    "\n",
    "if not dfs:\n",
    "    print(\"No files found!\")\n",
    "    exit()\n",
    "\n",
    "# --- HARD VOTING ---\n",
    "# We stack the predictions and take the \"Mode\" (Majority Vote)\n",
    "stacked_preds = np.array(dfs)\n",
    "# mode returns (values, counts), we just want the values [0]\n",
    "# axis=0 means looking down the columns (across the models)\n",
    "final_votes, _ = mode(stacked_preds, axis=0)\n",
    "\n",
    "# Flatten the array\n",
    "final_votes = final_votes.ravel()\n",
    "\n",
    "# --- SOFT VOTING (Alternative) ---\n",
    "# If you want to use average probability instead (often better):\n",
    "# avg_preds = np.mean(stacked_preds, axis=0)\n",
    "# final_votes = (avg_preds > 0.5).astype(int)\n",
    "\n",
    "# --- SAVE ---\n",
    "inverse_map = {0: 'Stayed', 1: 'Left'}\n",
    "submission_ensemble = pd.read_csv(os.path.join(OUTPUT_DIR, files[0])) # Load template\n",
    "submission_ensemble['retention_status'] = [inverse_map[p] for p in final_votes]\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, 'submission_Ensemble_Voting.csv')\n",
    "submission_ensemble.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Created Ensemble Submission: {output_path}\")\n",
    "print(\"This file combines the 'wisdom' of all your models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MlLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
