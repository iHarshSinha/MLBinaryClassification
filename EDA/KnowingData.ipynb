{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"       INTENSIVE DATA HEALTH CHECK       \")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Basic Info\n",
    "print(\"\\n--- 1. SHAPE & TYPES ---\")\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "print(df.info())\n",
    "\n",
    "# 2. Missing Values Deep Dive\n",
    "print(\"\\n--- 2. MISSING VALUES BREAKDOWN ---\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Percent %': missing_pct})\n",
    "print(missing_df[missing_df['Missing Count'] > 0].sort_values(by='Percent %', ascending=False))\n",
    "\n",
    "# 3. Duplicates\n",
    "print(\"\\n--- 3. DUPLICATE RECORDS ---\")\n",
    "print(f\"Duplicate Rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# 4. Cardinality (Unique Values)\n",
    "print(\"\\n--- 4. UNIQUE VALUES (CARDINALITY) ---\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# 5. Target Balance\n",
    "print(\"\\n--- 5. TARGET VARIABLE DISTRIBUTION ---\")\n",
    "print(df['retention_status'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# 6. Numerical Stats (Skewness check)\n",
    "print(\"\\n--- 6. NUMERICAL STATISTICS ---\")\n",
    "print(df.describe().T)\n",
    "\n",
    "print(\"\\n--- 7. CATEGORICAL BREAKDOWNS (Top 5 per col) ---\")\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    if col != 'retention_status':\n",
    "        print(f\"\\n[{col} Top 5 Categories]\")\n",
    "        print(df[col].value_counts().head(5))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"       END OF STATISTICAL REPORT       \")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81472ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Filter out ID column for plotting\n",
    "plot_df = df.drop(columns=['founder_id'], errors='ignore')\n",
    "\n",
    "# --- A. NUMERICAL VARIABLES ANALYSIS ---\n",
    "num_cols = plot_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# 1. Correlation Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(plot_df[num_cols].corr(), annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Master Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# 2. Distributions & Boxplots (Loop)\n",
    "for col in num_cols:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram (Distribution)\n",
    "    sns.histplot(data=plot_df, x=col, kde=True, ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title(f'Distribution of {col}')\n",
    "    \n",
    "    # Boxplot vs Target (Outliers & Separation)\n",
    "    sns.boxplot(data=plot_df, x='retention_status', y=col, hue='retention_status', legend=False, palette='coolwarm', ax=axes[1])\n",
    "    axes[1].set_title(f'{col} vs Retention Status')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- B. CATEGORICAL VARIABLES ANALYSIS ---\n",
    "cat_cols = plot_df.select_dtypes(include=['object']).columns\n",
    "# Remove target from this list so we don't plot it against itself\n",
    "cat_cols = [c for c in cat_cols if c != 'retention_status']\n",
    "\n",
    "for col in cat_cols:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Countplot split by Target\n",
    "    # If too many categories, rotate x-axis\n",
    "    if plot_df[col].nunique() > 10:\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "    sns.countplot(data=plot_df, x=col, hue='retention_status', palette='viridis')\n",
    "    plt.title(f'{col} Distribution by Retention Status')\n",
    "    plt.legend(title='Retention', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data (assuming you are in the same session)\n",
    "# df = pd.read_csv('data/train.csv') # Uncomment if you need to reload\n",
    "\n",
    "def clean_data(df):\n",
    "    # 1. Drop Duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # 2. Anomaly Removal\n",
    "    # Keep only founders 18 or older\n",
    "    df = df[df['founder_age'] >= 18]\n",
    "    \n",
    "    # Keep only valid tenures (started at age 16+)\n",
    "    df = df[(df['founder_age'] - df['years_with_startup']) >= 16]\n",
    "    \n",
    "    # 3. Missing Value Imputation\n",
    "    # Numerical: Median (robust to outliers)\n",
    "    num_cols = ['monthly_revenue_generated', 'years_since_founding', 'num_dependents']\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "    # Categorical: Treat \"Missing\" as \"Unknown\" for psychological surveys\n",
    "    # This captures the \"silence\" signal\n",
    "    df['work_life_balance_rating'] = df['work_life_balance_rating'].fillna('Unknown')\n",
    "    df['venture_satisfaction'] = df['venture_satisfaction'].fillna('Unknown')\n",
    "    \n",
    "    # For other structural columns, use Mode\n",
    "    df['team_size_category'] = df['team_size_category'].fillna(df['team_size_category'].mode()[0])\n",
    "    \n",
    "    # 4. Feature Engineering (Simplification)\n",
    "    # Create a \"Start Age\" feature to capture the age they started\n",
    "    df['start_age'] = df['founder_age'] - df['years_with_startup']\n",
    "    \n",
    "    # 5. Encoding Target\n",
    "    # Stayed -> 0, Left -> 1 (Standard for Churn/Exit prediction)\n",
    "    target_map = {'Stayed': 0, 'Left': 1}\n",
    "    df['target'] = df['retention_status'].map(target_map)\n",
    "    \n",
    "    # Drop unused columns\n",
    "    cols_to_drop = ['founder_id', 'retention_status']\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the cleaning\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "print(\"Data Cleaned Successfully!\")\n",
    "print(f\"New Shape: {df_clean.shape}\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81121fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def encode_and_split(df):\n",
    "    # --- 1. ORDINAL ENCODING (Preserving Order) ---\n",
    "    # We manually map these so the model knows 'Excellent' is better than 'Poor'\n",
    "    \n",
    "    rating_map = {'Unknown': 0, 'Low': 1, 'Poor': 1, 'Below Average': 2, \n",
    "                  'Fair': 3, 'Medium': 3, 'Average': 3, \n",
    "                  'Good': 4, 'High': 4, 'Very High': 5, 'Excellent': 5}\n",
    "    \n",
    "    # Apply to all rating-like columns\n",
    "    ord_cols = ['work_life_balance_rating', 'venture_satisfaction', \n",
    "                'startup_performance_rating', 'startup_reputation', 'founder_visibility']\n",
    "    \n",
    "    for col in ord_cols:\n",
    "        # Map and fill any unexpected values with 0 (Unknown)\n",
    "        df[col] = df[col].map(rating_map).fillna(0)\n",
    "\n",
    "    # Map Binary Columns (Yes/No)\n",
    "    binary_map = {'No': 0, 'Yes': 1}\n",
    "    bin_cols = ['working_overtime', 'remote_operations', 'innovation_support', 'leadership_scope']\n",
    "    for col in bin_cols:\n",
    "        df[col] = df[col].map(binary_map)\n",
    "\n",
    "    # Map Startup Stage (Roughly ordered)\n",
    "    stage_map = {'Entry': 1, 'Mid': 2, 'Senior': 3, 'Growth': 3, 'Established': 4}\n",
    "    df['startup_stage'] = df['startup_stage'].map(stage_map).fillna(1)\n",
    "\n",
    "    # --- 2. ONE-HOT ENCODING (Nominal Data) ---\n",
    "    # For Gender, Role, Education, etc.\n",
    "    # drop_first=True helps avoid multicollinearity (redundancy)\n",
    "    df = pd.get_dummies(df, columns=['founder_gender', 'founder_role', \n",
    "                                     'education_background', 'personal_status', \n",
    "                                     'team_size_category'], drop_first=True)\n",
    "\n",
    "    # --- 3. SPLITTING ---\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    # 80% Train, 20% Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # --- 4. SCALING ---\n",
    "    # Scale numerical features so age (e.g., 40) doesn't get overpowered by revenue (e.g., 5000)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to dataframe for readability (optional but helpful)\n",
    "    X_train_final = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "    X_test_final = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "    \n",
    "    return X_train_final, X_test_final, y_train, y_test\n",
    "\n",
    "# Execute\n",
    "X_train, X_test, y_train, y_test = encode_and_split(df_clean)\n",
    "\n",
    "print(\"Data Transformation Complete!\")\n",
    "print(f\"Training Features Shape: {X_train.shape}\")\n",
    "print(f\"Testing Features Shape: {X_test.shape}\")\n",
    "print(\"\\nFirst 5 rows of processed data (All Numbers Now!):\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Logistic Regression (The Baseline) ---\n",
    "print(\"Training Logistic Regression...\")\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "# --- 2. Random Forest (The Complex Model) ---\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# --- 3. Evaluation Helper Function ---\n",
    "def evaluate_model(y_test, y_pred, model_name):\n",
    "    print(\"=\"*40)\n",
    "    print(f\"   {model_name} PERFORMANCE\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Basic Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy Score: {acc:.2%}\")\n",
    "    \n",
    "    # Detailed Report (Precision/Recall)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Stayed (0)', 'Left (1)']))\n",
    "    \n",
    "    # Visual Confusion Matrix\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name}: Confusion Matrix')\n",
    "    plt.ylabel('Actual Truth')\n",
    "    plt.xlabel('Model Prediction')\n",
    "    plt.show()\n",
    "\n",
    "# Run the comparisons\n",
    "evaluate_model(y_test, y_pred_log, \"Logistic Regression\")\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# Feature Importance (What mattered most?)\n",
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "print(\"\\n--- Top 10 Factors Driving Retention ---\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Starting Grid Search... (This may take a few minutes)\")\n",
    "\n",
    "# 1. Define the Parameter Grid (The \"Knobs\" to turn)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],        # How many trees?\n",
    "    'max_depth': [10, 20, None],       # How deep can they grow?\n",
    "    'min_samples_split': [2, 5, 10],   # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]      # Minimum samples at the end of a branch\n",
    "}\n",
    "\n",
    "# 2. Initialize Grid Search\n",
    "# cv=3 means \"Cross-Validation\": it splits training data 3 ways to verify results internally\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# 3. Fit to Training Data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 4. Get the Best Model\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 5. Evaluate the Tuned Model\n",
    "print(\"\\nEvaluating Tuned Model on Test Set...\")\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate Metrics\n",
    "acc = accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"Tuned Random Forest Accuracy: {acc:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned, target_names=['Stayed (0)', 'Left (1)']))\n",
    "\n",
    "# 6. Compare with Baseline (Visual)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_tuned), annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(f'Tuned Random Forest (Acc: {acc:.2%})')\n",
    "plt.ylabel('Actual Truth')\n",
    "plt.xlabel('Model Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f298b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINAL SUBMISSION PIPELINE ---\n",
    "\n",
    "# 1. Load Data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sample_sub = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "# Combine to ensure One-Hot Encoding is identical for both\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test['retention_status'] = 'Unknown' # Placeholder\n",
    "combined = pd.concat([train, test], axis=0)\n",
    "\n",
    "# 2. Apply the EXACT same cleaning as before\n",
    "def process_data(df):\n",
    "    # Drop duplicates (Only for train, be careful with test IDs!)\n",
    "    # Actually, for submission, we usually keep test rows as-is to match IDs.\n",
    "    \n",
    "    # Cleaning\n",
    "    df['founder_age'] = df['founder_age'].clip(lower=18) # Fix underage instead of dropping\n",
    "    \n",
    "    # Imputing\n",
    "    num_cols = ['monthly_revenue_generated', 'years_since_founding', 'num_dependents']\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "    df['work_life_balance_rating'] = df['work_life_balance_rating'].fillna('Unknown')\n",
    "    df['venture_satisfaction'] = df['venture_satisfaction'].fillna('Unknown')\n",
    "    df['team_size_category'] = df['team_size_category'].fillna(df['team_size_category'].mode()[0])\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df['start_age'] = df['founder_age'] - df['years_with_startup']\n",
    "    \n",
    "    # Ordinal Encoding\n",
    "    rating_map = {'Unknown': 0, 'Low': 1, 'Poor': 1, 'Below Average': 2, \n",
    "                  'Fair': 3, 'Medium': 3, 'Average': 3, \n",
    "                  'Good': 4, 'High': 4, 'Very High': 5, 'Excellent': 5}\n",
    "    ord_cols = ['work_life_balance_rating', 'venture_satisfaction', \n",
    "                'startup_performance_rating', 'startup_reputation', 'founder_visibility']\n",
    "    for col in ord_cols:\n",
    "        df[col] = df[col].map(rating_map).fillna(0)\n",
    "\n",
    "    binary_map = {'No': 0, 'Yes': 1}\n",
    "    bin_cols = ['working_overtime', 'remote_operations', 'innovation_support', 'leadership_scope']\n",
    "    for col in bin_cols:\n",
    "        df[col] = df[col].map(binary_map)\n",
    "\n",
    "    stage_map = {'Entry': 1, 'Mid': 2, 'Senior': 3, 'Growth': 3, 'Established': 4}\n",
    "    df['startup_stage'] = df['startup_stage'].map(stage_map).fillna(1)\n",
    "    \n",
    "    # One-Hot Encoding\n",
    "    df = pd.get_dummies(df, columns=['founder_gender', 'founder_role', \n",
    "                                     'education_background', 'personal_status', \n",
    "                                     'team_size_category'], drop_first=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process the combined dataframe\n",
    "combined_processed = process_data(combined)\n",
    "\n",
    "# 3. Split back into Train and Test\n",
    "train_final = combined_processed[combined_processed['is_train'] == 1].drop(columns=['is_train', 'founder_id'])\n",
    "test_final = combined_processed[combined_processed['is_train'] == 0].drop(columns=['is_train', 'retention_status'])\n",
    "\n",
    "# Target preparation\n",
    "target_map = {'Stayed': 0, 'Left': 1}\n",
    "y = train_final['retention_status'].map(target_map)\n",
    "X = train_final.drop(columns=['retention_status'])\n",
    "\n",
    "# Ensure Test has same columns as Train\n",
    "X_test_submit = test_final[X.columns] # Reorder/filter columns\n",
    "\n",
    "# 4. Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_submit_scaled = scaler.transform(X_test_submit)\n",
    "\n",
    "# 5. Train the BEST Model (Tuned Random Forest)\n",
    "final_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Training Final Model on Full Data...\")\n",
    "final_model.fit(X_scaled, y)\n",
    "\n",
    "# 6. Predict\n",
    "predictions = final_model.predict(X_test_submit_scaled)\n",
    "\n",
    "# 7. Create Submission File\n",
    "# Map back to original labels if required, or use 0/1? \n",
    "# Sample submission usually wants the class label. Let's check sample_sub.\n",
    "# The prompt showed: \"Left\" and \"Stayed\". We must map back!\n",
    "\n",
    "inverse_map = {0: 'Stayed', 1: 'Left'}\n",
    "submission_df = pd.DataFrame({\n",
    "    'founder_id': test['founder_id'],\n",
    "    'retention_status': [inverse_map[p] for p in predictions]\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Success! 'submission.csv' saved.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Training Gradient Boosting Model...\")\n",
    "\n",
    "# 1. Initialize the Model\n",
    "# key params: learning_rate (how fast it learns), n_estimators (trees), max_depth (keep small!)\n",
    "gbm = GradientBoostingClassifier(\n",
    "    n_estimators=200, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Train\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict\n",
    "y_pred_gbm = gbm.predict(X_test)\n",
    "\n",
    "# 4. Evaluate\n",
    "gbm_acc = accuracy_score(y_test, y_pred_gbm)\n",
    "print(f\"Gradient Boosting Accuracy: {gbm_acc:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gbm, target_names=['Stayed (0)', 'Left (1)']))\n",
    "\n",
    "# 5. Visual Comparison (GBM Confusion Matrix)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_gbm), annot=True, fmt='d', cmap='Oranges')\n",
    "plt.title(f'Gradient Boosting (Acc: {gbm_acc:.2%})')\n",
    "plt.ylabel('Actual Truth')\n",
    "plt.xlabel('Model Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SUBMISSION WITH GBM ---\n",
    "\n",
    "print(\"Retraining GBM on Full Data and generating submission...\")\n",
    "\n",
    "# 1. Train on ALL data (X_scaled from previous step)\n",
    "final_gbm = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "final_gbm.fit(X_scaled, y)\n",
    "\n",
    "# 2. Predict on Test Set\n",
    "gbm_predictions = final_gbm.predict(X_test_submit_scaled)\n",
    "\n",
    "# 3. Save File\n",
    "inverse_map = {0: 'Stayed', 1: 'Left'}\n",
    "submission_gbm = pd.DataFrame({\n",
    "    'founder_id': test['founder_id'],\n",
    "    'retention_status': [inverse_map[p] for p in gbm_predictions]\n",
    "})\n",
    "\n",
    "submission_gbm.to_csv('submission_gbm.csv', index=False)\n",
    "print(\"Success! 'submission_gbm.csv' saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- 1. Support Vector Machine (SVM) ---\n",
    "print(\"Training SVM... (This might take a few minutes, grab a coffee â˜•)\")\n",
    "# kernel='rbf' allows it to fit non-linear curves\n",
    "# C=1.0 is the penalty parameter (Higher C = strict margin, Lower C = soft margin)\n",
    "svm_model = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "svm_model.fit(X_train, y_train) # Make sure to use the SCALED X_train!\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_acc:.2%}\")\n",
    "\n",
    "# --- 2. Neural Network (MLP) ---\n",
    "print(\"\\nTraining Neural Network...\")\n",
    "# hidden_layer_sizes=(64, 32): Two layers with 64 and 32 neurons\n",
    "# max_iter=500: Allow it enough time to learn (converge)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(64, 32), \n",
    "                          activation='relu', \n",
    "                          solver='adam', \n",
    "                          max_iter=500, \n",
    "                          random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "mlp_acc = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f\"Neural Network Accuracy: {mlp_acc:.2%}\")\n",
    "\n",
    "# --- 3. Quick Comparison Report ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"     MODEL LEADERBOARD     \")\n",
    "print(\"=\"*30)\n",
    "print(f\"Gradient Boosting (Prev):  (Check your previous output)\") \n",
    "print(f\"Random Forest (Tuned):     74.16%\")\n",
    "print(f\"Support Vector Machine:    {svm_acc:.2%}\")\n",
    "print(f\"Neural Network (MLP):      {mlp_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0781ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM on Full Data... (This will take a few minutes)\n",
      "Success! 'submission_svm.csv' saved.\n",
      "\n",
      "Training Neural Network on Full Data...\n",
      "Success! 'submission_neural_network.csv' saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Note: This assumes 'X_scaled', 'y', 'X_test_submit_scaled', and 'test' \n",
    "# are already defined from the previous data preparation step.\n",
    "\n",
    "inverse_map = {0: 'Stayed', 1: 'Left'}\n",
    "\n",
    "# --- 1. SUBMISSION FOR SVM ---\n",
    "print(\"Training SVM on Full Data... (This will take a few minutes)\")\n",
    "final_svm = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "final_svm.fit(X_scaled, y)\n",
    "\n",
    "# Predict\n",
    "svm_predictions = final_svm.predict(X_test_submit_scaled)\n",
    "\n",
    "# Save\n",
    "submission_svm = pd.DataFrame({\n",
    "    'founder_id': test['founder_id'],\n",
    "    'retention_status': [inverse_map[p] for p in svm_predictions]\n",
    "})\n",
    "submission_svm.to_csv('submission_svm.csv', index=False)\n",
    "print(\"Success! 'submission_svm.csv' saved.\")\n",
    "\n",
    "\n",
    "# --- 2. SUBMISSION FOR NEURAL NETWORK ---\n",
    "print(\"\\nTraining Neural Network on Full Data...\")\n",
    "final_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32), \n",
    "    activation='relu', \n",
    "    solver='adam', \n",
    "    max_iter=500, \n",
    "    random_state=42\n",
    ")\n",
    "final_mlp.fit(X_scaled, y)\n",
    "\n",
    "# Predict\n",
    "mlp_predictions = final_mlp.predict(X_test_submit_scaled)\n",
    "\n",
    "# Save\n",
    "submission_mlp = pd.DataFrame({\n",
    "    'founder_id': test['founder_id'],\n",
    "    'retention_status': [inverse_map[p] for p in mlp_predictions]\n",
    "})\n",
    "submission_mlp.to_csv('submission_neural_network.csv', index=False)\n",
    "print(\"Success! 'submission_neural_network.csv' saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MlLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
